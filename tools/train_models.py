#!/usr/bin/env python3
"""
NINA ML Model Training Pipeline v2.0 — REAL DATA VERSION

Reads REAL blockchain features from ml_training_data.csv (generated by the daemon's
BlockDataLogger) and trains models on actual observed block behavior.

PHASE 1: Isolation Forest for anomaly detection (unsupervised — no fake labels needed)
PHASE 2: XGBoost for difficulty prediction (supervised — predict next solve_time)

Input:  ~/.ninacatcoin/ml_training_data.csv (real daemon output)
Output: Trained models in src/nina_ml/models/
"""

import os
import sys
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging

# ML libraries
from sklearn.ensemble import IsolationForest, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import (mean_squared_error, mean_absolute_error, 
                             mean_absolute_percentage_error)
import joblib

# Try xgboost (better for time series), fall back to sklearn GBR
try:
    from xgboost import XGBRegressor
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# Paths
if sys.platform != 'win32':
    import pwd
    home_dir = Path(pwd.getpwnam(os.getenv('USER', 'jose')).pw_dir)
else:
    home_dir = Path(os.path.expandvars('%USERPROFILE%'))

BLOCKCHAIN_DIR = home_dir / '.ninacatcoin'
TRAINING_CSV = BLOCKCHAIN_DIR / 'ml_training_data.csv'
MODELS_DIR = Path(__file__).parent / 'src' / 'nina_ml' / 'models'

logger.info(f"Blockchain dir: {BLOCKCHAIN_DIR}")
logger.info(f"Training CSV:   {TRAINING_CSV}")
logger.info(f"Models dir:     {MODELS_DIR}")

# ============================================================================
# DATA LOADING — reads REAL data from daemon CSV
# ============================================================================

def load_training_data():
    """Load real block features from BlockDataLogger CSV output"""
    
    if not TRAINING_CSV.exists():
        logger.error(f"Training data not found: {TRAINING_CSV}")
        logger.error("The daemon must run with BlockDataLogger enabled to generate this file.")
        logger.info("Hint: The file is auto-generated at ~/.ninacatcoin/ml_training_data.csv")
        return None
    
    logger.info(f"Loading training data from {TRAINING_CSV}...")
    df = pd.read_csv(TRAINING_CSV)
    
    logger.info(f"✓ Loaded {len(df)} blocks")
    logger.info(f"  Height range: {df['height'].min()} - {df['height'].max()}")
    logger.info(f"  Columns: {list(df.columns)}")
    
    if len(df) < 100:
        logger.warning(f"Only {len(df)} blocks — models may be unreliable. Recommend >500 blocks.")
    
    before = len(df)
    df = df.dropna()
    if len(df) < before:
        logger.warning(f"Dropped {before - len(df)} rows with missing values")
    
    return df


# ============================================================================
# FEATURE ENGINEERING — sliding window features from real data
# ============================================================================

def engineer_features(df):
    """
    Create sliding window features from raw block data.
    These capture temporal patterns that simple per-block features miss.
    """
    logger.info("Engineering sliding window features...")
    
    df = df.sort_values('height').reset_index(drop=True)
    
    for window in [10, 30, 60]:
        col_prefix = f'w{window}'
        df[f'{col_prefix}_solve_mean'] = df['solve_time'].rolling(window, min_periods=1).mean()
        df[f'{col_prefix}_solve_std'] = df['solve_time'].rolling(window, min_periods=1).std().fillna(0)
        df[f'{col_prefix}_solve_median'] = df['solve_time'].rolling(window, min_periods=1).median()
        df[f'{col_prefix}_diff_mean'] = df['difficulty'].rolling(window, min_periods=1).mean()
        df[f'{col_prefix}_diff_std'] = df['difficulty'].rolling(window, min_periods=1).std().fillna(0)
        df[f'{col_prefix}_health_mean'] = df['network_health'].rolling(window, min_periods=1).mean()
    
    df['diff_change_pct'] = df['difficulty'].pct_change().fillna(0) * 100.0
    df['solve_time_change'] = df['solve_time'].diff().fillna(0)
    df['solve_deviation'] = df['solve_time'] - 120.0
    df['solve_deviation_abs'] = df['solve_deviation'].abs()
    df['diff_vs_avg_ratio'] = df['difficulty'] / df['w30_diff_mean'].replace(0, 1)
    df['solve_cv_30'] = df['w30_solve_std'] / df['w30_solve_mean'].replace(0, 1)
    
    df = df.iloc[60:].reset_index(drop=True)
    
    logger.info(f"✓ Engineered {len(df.columns)} features over {len(df)} blocks")
    return df


# ============================================================================
# PHASE 1: Isolation Forest for Anomaly Detection (UNSUPERVISED)
# ============================================================================

def train_phase1_anomaly_detector(df):
    """
    Train Isolation Forest on REAL block features.
    UNSUPERVISED — learns what "normal" looks like, flags deviations.
    """
    logger.info("=" * 60)
    logger.info("PHASE 1: Training Anomaly Detector (Isolation Forest)")
    logger.info("=" * 60)
    
    feature_cols = [
        'network_health', 'miner_diversity', 'hash_entropy',
        'solve_time', 'solve_deviation_abs', 'solve_cv_30',
        'diff_change_pct', 'diff_vs_avg_ratio',
        'w10_solve_std', 'w30_health_mean'
    ]
    
    available_cols = [c for c in feature_cols if c in df.columns]
    if len(available_cols) < 5:
        logger.error(f"Not enough features for PHASE 1 (have {len(available_cols)}, need >=5)")
        return None, None
    
    X = df[available_cols].values
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    from sklearn.ensemble import IsolationForest
    model = IsolationForest(
        n_estimators=200,
        contamination=0.05,
        max_samples='auto',
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_scaled)
    
    scores = model.decision_function(X_scaled)
    predictions = model.predict(X_scaled)
    n_anomalies = (predictions == -1).sum()
    
    logger.info(f"✓ PHASE 1 trained on {len(X)} blocks")
    logger.info(f"  Features: {available_cols}")
    logger.info(f"  Anomalies: {n_anomalies} ({n_anomalies/len(X)*100:.1f}%)")
    logger.info(f"  Score range: [{scores.min():.3f}, {scores.max():.3f}]")
    
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, MODELS_DIR / 'phase1_anomaly_detector.pkl')
    joblib.dump(scaler, MODELS_DIR / 'phase1_scaler.pkl')
    joblib.dump(available_cols, MODELS_DIR / 'phase1_features.pkl')
    
    logger.info(f"✓ Saved to {MODELS_DIR / 'phase1_anomaly_detector.pkl'}")
    return model, scaler


# ============================================================================
# PHASE 2: Difficulty Prediction (SUPERVISED — time-series validated)
# ============================================================================

def train_phase2_difficulty_predictor(df):
    """
    Train model to predict difficulty adjustment multiplier.
    Uses time-series cross-validation to avoid lookahead bias.
    """
    logger.info("=" * 60)
    logger.info("PHASE 2: Training Difficulty Predictor")
    logger.info("=" * 60)
    
    feature_cols = [
        'solve_time', 'difficulty', 'network_health',
        'w10_solve_mean', 'w10_solve_std',
        'w30_solve_mean', 'w30_solve_std', 'w30_diff_mean',
        'w60_solve_mean', 'w60_diff_mean',
        'diff_change_pct', 'solve_deviation', 'diff_vs_avg_ratio',
        'solve_cv_30'
    ]
    
    available_cols = [c for c in feature_cols if c in df.columns]
    if len(available_cols) < 5:
        logger.error(f"Not enough features for PHASE 2 (have {len(available_cols)}, need >=5)")
        return None, None
    
    X = df[available_cols].values
    target_time = 120.0
    y = np.clip(target_time / np.maximum(df['solve_time'].values, 1.0), 0.5, 2.0)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    from sklearn.model_selection import TimeSeriesSplit
    tscv = TimeSeriesSplit(n_splits=5)
    rmse_scores = []
    
    for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):
        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        fold_model = GradientBoostingRegressor(
            n_estimators=200, learning_rate=0.05, max_depth=5,
            subsample=0.8, random_state=42
        )
        fold_model.fit(X_train, y_train)
        y_pred = fold_model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        rmse_scores.append(rmse)
        logger.info(f"  Fold {fold+1}: RMSE={rmse:.4f}")
    
    logger.info(f"  CV RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}")
    
    model = GradientBoostingRegressor(
        n_estimators=200, learning_rate=0.05, max_depth=5,
        subsample=0.8, random_state=42
    )
    model.fit(X_scaled, y)
    
    importances = model.feature_importances_
    for feat, imp in sorted(zip(available_cols, importances), key=lambda x: -x[1]):
        logger.info(f"  Feature importance: {feat:>25s} = {imp:.4f}")
    
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, MODELS_DIR / 'phase2_difficulty.pkl')
    joblib.dump(scaler, MODELS_DIR / 'phase2_scaler.pkl')
    joblib.dump(available_cols, MODELS_DIR / 'phase2_features.pkl')
    
    logger.info(f"✓ PHASE 2 (GradientBoosting) saved")
    return model, scaler


# ============================================================================
# MAIN
# ============================================================================

def main():
    logger.info("=" * 70)
    logger.info("NINA ML TRAINING PIPELINE v2.0 — REAL DATA")
    logger.info("=" * 70)
    
    df = load_training_data()
    if df is None:
        return False
    
    if len(df) < 100:
        logger.error(f"Need at least 100 blocks to train (have {len(df)})")
        return False
    
    df = engineer_features(df)
    
    if len(df) < 50:
        logger.error(f"After windowing, only {len(df)} samples remain (need >=50)")
        return False
    
    model1, scaler1 = train_phase1_anomaly_detector(df)
    model2, scaler2 = train_phase2_difficulty_predictor(df)
    
    logger.info("=" * 70)
    logger.info("TRAINING COMPLETE")
    logger.info(f"  Samples: {len(df)} blocks")
    logger.info(f"  PHASE 1: {'✓ Isolation Forest' if model1 else '✗ Failed'}")
    logger.info(f"  PHASE 2: {'✓ GradientBoosting' if model2 else '✗ Failed'}")
    logger.info(f"  Models: {MODELS_DIR}")
    logger.info("=" * 70)
    
    return model1 is not None and model2 is not None


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
